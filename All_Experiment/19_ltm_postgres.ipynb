{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d336fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install -U \"psycopg[binary,pool]\" langgraph langgraph-checkpoint-postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78ca5170",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "import uuid\n",
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END, MessagesState\n",
    "from langgraph.store.postgres import PostgresStore\n",
    "from langgraph.store.base import BaseStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66911b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 2) System prompt\n",
    "# ----------------------------\n",
    "SYSTEM_PROMPT_TEMPLATE = \"\"\"You are a helpful assistant with memory capabilities.\n",
    "If user-specific memory is available, use it to personalize \n",
    "your responses based on what you know about the user.\n",
    "\n",
    "Your goal is to provide relevant, friendly, and tailored \n",
    "assistance that reflects the user’s preferences, context, and past interactions.\n",
    "\n",
    "If the user’s name or relevant personal context is available, always personalize your responses by:\n",
    "    – Always Address the user by name (e.g., \"Sure, Nitish...\") when appropriate\n",
    "    – Referencing known projects, tools, or preferences (e.g., \"your MCP server python based project\")\n",
    "    – Adjusting the tone to feel friendly, natural, and directly aimed at the user\n",
    "\n",
    "Avoid generic phrasing when personalization is possible.\n",
    "\n",
    "Use personalization especially in:\n",
    "    – Greetings and transitions\n",
    "    – Help or guidance tailored to tools and frameworks the user uses\n",
    "    – Follow-up messages that continue from past context\n",
    "\n",
    "Always ensure that personalization is based only on known user details and not assumed.\n",
    "\n",
    "In the end suggest 3 relevant further questions based on the current response and user profile\n",
    "\n",
    "The user’s memory (which may be empty) is provided as: {user_details_content}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11253a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 3) Memory extraction LLM\n",
    "# ----------------------------\n",
    "import os\n",
    "def get_groq_llm():\n",
    "    return ChatOpenAI(\n",
    "        model=\"openai/gpt-oss-20b\",\n",
    "        base_url=\"https://api.groq.com/openai/v1\",\n",
    "        api_key=os.getenv(\"GROQ_API_KEY\"),\n",
    "        temperature=0.7, max_tokens=2000\n",
    "    )\n",
    "\n",
    "memory_llm = get_groq_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e74508e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryItem(BaseModel):\n",
    "    text: str = Field(description=\"Atomic user memory\")\n",
    "    is_new: bool = Field(description=\"True if new, false if duplicate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6145118a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryDecision(BaseModel):\n",
    "    should_write: bool\n",
    "    memories: List[MemoryItem] = Field(default_factory=list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60e1b1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_extractor = memory_llm.with_structured_output(MemoryDecision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "101b07cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "MEMORY_PROMPT = \"\"\"You are responsible for updating and maintaining accurate user memory.\n",
    "\n",
    "CURRENT USER DETAILS (existing memories):\n",
    "{user_details_content}\n",
    "\n",
    "TASK:\n",
    "- Review the user's latest message.\n",
    "- Extract user-specific info worth storing long-term (identity, stable preferences, ongoing projects/goals).\n",
    "- For each extracted item, set is_new=true ONLY if it adds NEW information compared to CURRENT USER DETAILS.\n",
    "- If it is basically the same meaning as something already present, set is_new=false.\n",
    "- Keep each memory as a short atomic sentence.\n",
    "- No speculation; only facts stated by the user.\n",
    "- If there is nothing memory-worthy, return should_write=false and an empty list.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f0b058e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 3) Nodes\n",
    "# ----------------------------\n",
    "def remember_node(state: MessagesState, config: RunnableConfig, *, store: BaseStore):\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "    ns = (\"user\", user_id, \"details\")\n",
    "\n",
    "    # existing memory (all items under namespace)\n",
    "    items = store.search(ns)\n",
    "    existing = \"\\n\".join(it.value.get(\"data\", \"\") for it in items) if items else \"(empty)\"\n",
    "\n",
    "    # latest user message\n",
    "    last_text = state[\"messages\"][-1].content\n",
    "\n",
    "    decision: MemoryDecision = memory_extractor.invoke(\n",
    "        [\n",
    "            SystemMessage(content=MEMORY_PROMPT.format(user_details_content=existing)),\n",
    "            {\"role\": \"user\", \"content\": last_text},\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if decision.should_write:\n",
    "        for mem in decision.memories:\n",
    "            if mem.is_new and mem.text.strip():\n",
    "                store.put(ns, str(uuid.uuid4()), {\"data\": mem.text.strip()})\n",
    "\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5bf7c145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_groq_llm():\n",
    "    return ChatOpenAI(\n",
    "        model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "        base_url=\"https://api.groq.com/openai/v1\",\n",
    "        api_key=os.getenv(\"GROQ_API_KEY\"),\n",
    "        temperature=0.7, max_tokens=2000\n",
    "    )\n",
    "\n",
    "chat_llm = get_groq_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e9107c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_node(state: MessagesState, config: RunnableConfig, *, store: BaseStore):\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "    ns = (\"user\", user_id, \"details\")\n",
    "\n",
    "    items = store.search(ns)\n",
    "    user_details = \"\\n\".join(it.value.get(\"data\", \"\") for it in items) if items else \"\"\n",
    "\n",
    "    system_msg = SystemMessage(\n",
    "        content=SYSTEM_PROMPT_TEMPLATE.format(user_details_content=user_details or \"(empty)\")\n",
    "    )\n",
    "\n",
    "    response = chat_llm.invoke([system_msg] + state[\"messages\"])\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "77457bb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x122143c50>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 4) Build graph\n",
    "# ----------------------------\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"remember\", remember_node)\n",
    "builder.add_node(\"chat\", chat_node)\n",
    "builder.add_edge(START, \"remember\")\n",
    "builder.add_edge(\"remember\", \"chat\")\n",
    "builder.add_edge(\"chat\", END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d8f3d3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sachin Mishra, I'm happy to help you understand GenAI in simple terms while you're teaching langgraph.\n",
      "\n",
      "GenAI refers to a type of artificial intelligence designed to generate human-like content, such as text, images, or music. The term \"Gen\" in GenAI likely stands for \"Generative,\" indicating its primary function: to create new, original content.\n",
      "\n",
      "Think of GenAI like a creative tool that can produce various forms of media. For instance, it can:\n",
      "\n",
      "1. Write articles, stories, or even entire books.\n",
      "2. Generate images, like artwork or graphics.\n",
      "3. Compose music.\n",
      "\n",
      "The goal of GenAI is to automate content creation, making it faster and more efficient. However, it's essential to note that GenAI models require large amounts of data to learn and improve, and their output might need refinement to ensure accuracy and quality.\n",
      "\n",
      "As you're teaching langgraph, you might be interested in exploring how GenAI can be integrated with language models like langgraph to generate human-like text or conversation.\n",
      "\n",
      "Here are three further questions to consider:\n",
      "\n",
      "1. How can GenAI be used to enhance langgraph's conversational capabilities?\n",
      "2. What are the potential applications of GenAI in content creation, and how can langgraph be used to support these use cases?\n",
      "3. What are the challenges and limitations of using GenAI, and how can they be addressed in a langgraph implementation?\n",
      "\n",
      "--- Stored Memories (from Postgres) ---\n",
      "User is teaching langgraph\n",
      "User's name is Sachin Mishra\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 5) Use PostgresStore (PERSISTENT LTM)\n",
    "# ----------------------------\n",
    "DB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\"\n",
    "\n",
    "with PostgresStore.from_conn_string(DB_URI) as store:\n",
    "    # IMPORTANT: run ONCE the first time you use this database\n",
    "    store.setup()\n",
    "\n",
    "    graph = builder.compile(store=store)\n",
    "\n",
    "    config = {\"configurable\": {\"user_id\": \"u1\"}}\n",
    "\n",
    "    graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Hi, my name is Sachin Mishra\"}]}, config)\n",
    "    graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"I am teaching langgraph for now\"}]}, config)\n",
    "\n",
    "    out = graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Explain GenAI simply\"}]}, config)\n",
    "    print(out[\"messages\"][-1].content)\n",
    "\n",
    "    print(\"\\n--- Stored Memories (from Postgres) ---\")\n",
    "    for it in store.search((\"user\", \"u1\", \"details\")):\n",
    "        print(it.value[\"data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e0f802",
   "metadata": {},
   "source": [
    "## Check Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e305c173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User is teaching langgraph\n",
      "User's name is Sachin Mishra\n"
     ]
    }
   ],
   "source": [
    "from langgraph.store.postgres import PostgresStore\n",
    "\n",
    "DB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\"\n",
    "\n",
    "with PostgresStore.from_conn_string(DB_URI) as store:\n",
    "    ns = (\"user\", \"u1\", \"details\")\n",
    "    items = store.search(ns)\n",
    "\n",
    "for it in items:\n",
    "    print(it.value[\"data\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083b6600",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph-project-based-learning (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
